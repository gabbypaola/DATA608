theme_minimal()
DataExplorer::plot_histogram(test_features, nrow = 3L, ncol = 4L)
#NAs in Test data set
#NA counts by column
#sapply(test, function(x) sum(is.na(x)))
VIM::aggr(test_features, numbers=T, sortVars=T, bars = FALSE, border= 'white',
cex.axis = .6,
ylab=c("Proportion of NAs", "Combinations"))
#prep data- change Brand.Code to factor
#add name to empty string
test_features$Brand.Code[test_features$Brand.Code == ""] <- "Unknown"
#convert variable type to factor
test_features <- test_features %>%
dplyr::mutate(Brand.Code = factor(Brand.Code,
levels = c('A','B','C','D', 'Unknown'),
ordered = FALSE))
set.seed(624)
#impute NAs
preProc_test <- preProcess(test_features, method=c("knnImpute"))
#get the transformed features
test_features <- predict(preProc_test, test_features)
#add the PH response variable to the preProcessed train features
#$PH <- test$PH
#preProc_test_features$PH <- test$PH
setdiff(colnames(test_features),colnames(train_rf_predictors))
test_features <- test_features %>%
dplyr::select(-c("Hyd.Pressure1","Hyd.Pressure3","Filler.Level",
"Filler.Speed","Density","Balling","Balling.Lvl" ))
str(train_rf_predictors)
colnames(train_rf_predictors)
all_metrics <- as.data.frame(rbind("Linear Regression" = lmResample,
"Stepwise AIC" = aicResample,
"Partial Least Squares" = plsReSample,
"KNN" = knn_metrics,
"SVM" = svm_metrics,
"MARS" = mars_metrics,
"Neural Network" = nn_metrics,
"Random Forest" = rf_metrics,
) )
all_metrics <- as.data.frame(rbind("Linear Regression" = lmResample,
"Stepwise AIC" = aicResample,
"Partial Least Squares" = plsReSample,
"KNN" = knn_metrics,
"SVM" = svm_metrics,
"MARS" = mars_metrics,
"Neural Network" = nn_metrics,
"Random Forest" = rf_metrics
) )
all_metrics[order(all_metrics$RMSE),] %>%
kable() %>% kable_paper()
all_metrics <- as.data.frame(rbind("Linear Regression" = lmResample,
"Stepwise AIC" = aicResample,
"Partial Least Squares" = plsReSample,
"Neural Network" = nn_metrics,
"Random Forest" = rf_metrics
) )
all_metrics[order(all_metrics$RMSE),] %>%
kable() %>% kable_paper()
all_metrics <- as.data.frame(rbind("Linear Regression" = lmResample,
"Neural Network" = nn_metrics,
"Random Forest" = rf_metrics
))
all_metrics[order(all_metrics$RMSE),] %>%
kable() %>% kable_paper()
plot(varImp(rf_model), top = 10)
library(tibble)
imp_vars <- varImp(rf_model) %>% head(10)
imp_vars <- tibble::rownames_to_column(imp_vars, "Variable")
ggplot(imp_vars,aes(x=fct_reorder(factor(Variable),Overall), y = (Overall))) +
geom_col(position="dodge", fill="steelblue") +
coord_flip() +
labs(title="Random Forest Model",
subtitle = "Top Ten Important Predictors",
x="Predictors",
y="Variable Importance") +
theme_minimal()
set.seed(100)
cube_model <- train(train_rf_predictors, train_data_rf$PH,
method = "cubist",
verbose = FALSE)
cubePred <- predict(cube_model, newdata =  eval_rf_predictors)
cube_metrics <- postResample(pred = cubePred, obs = eval_data_rf$PH)
cube_metrics
rbind("Random Forest" = rf_metrics,
"Boosted Trees" = gbm_metrics,
"Cubist" = cube_metrics) %>%
kable() %>% kable_paper()
gbm_model <- train(train_rf_predictors, train_data_rf$PH,
method = "gbm",
verbose = FALSE)
gbmPred <- predict(gbm_model, newdata = eval_rf_predictors)
gbm_metrics <- postResample(pred = gbmPred, obs = eval_data_rf$PH)
gbm_model <- train(train_rf_predictors, train_data_rf$PH,
method = "gbm",
verbose = FALSE)
gbmPred <- predict(gbm_model, newdata = eval_rf_predictors)
gbm_metrics <- postResample(pred = gbmPred, obs = eval_data_rf$PH)
gbm_metrics
set.seed(624)
#fit the model
rf_model <- randomForest(train_rf_predictors, train_data_rf$PH,
importance = TRUE, ntrees = 1000)
rf_model
rfPred <- predict(rf_model, newdata = eval_rf_predictors)
rf_metrics <- postResample(pred = rfPred, obs = eval_data_rf$PH)
#rf_metrics
set.seed(624)
gbm_model <- train(train_rf_predictors, train_data_rf$PH,
method = "gbm",
verbose = FALSE)
gbmPred <- predict(gbm_model, newdata = eval_rf_predictors)
gbm_metrics <- postResample(pred = gbmPred, obs = eval_data_rf$PH)
#gbm_metrics
set.seed(624)
cube_model <- train(train_rf_predictors, train_data_rf$PH,
method = "cubist",
verbose = FALSE)
cubePred <- predict(cube_model, newdata =  eval_rf_predictors)
cube_metrics <- postResample(pred = cubePred, obs = eval_data_rf$PH)
#cube_metrics
rbind("Random Forest" = rf_metrics,
"Boosted Trees" = gbm_metrics,
"Cubist" = cube_metrics) %>%
kable() %>% kable_paper()
library(tibble)
imp_vars <- varImp(cube_model) %>% head(10)
imp_vars <- tibble::rownames_to_column(imp_vars, "Variable")
varImp(cube_model)
varImp(cube_model)%>% head(10)
varImp(cube_model)%>% head(10)
plot(varImp(cube_model), top=10)
predict <- round(predict(cube_model, newdata=test_features),2)
```{r}
predict <- round(predict(cube_model, newdata=test_features),2)
test_features$PH <- predict
View(test_features)
View(test_features)
write_excel_csv(test_features, "Predictions.csv")
plot(varImp(NNModel), top_n=10)
set.seed(624)
#fit the model
rf_model <- randomForest(train_rf_predictors, train_data_rf$PH,
importance = TRUE, ntrees = 500)
rf_model
rfPred <- predict(rf_model, newdata = eval_rf_predictors)
rf_metrics <- postResample(pred = rfPred, obs = eval_data_rf$PH)
#rf_metrics
library(ggplot2)
library(readr)
library(DataExplorer)
library(summarytools)
library(Amelia)
library(VIM)
library(dplyr)
library(forecast)
library(tidyr)
library(mice)
library(corrplot)
library(MASS)
library(earth)
library(RANN)
library(caret)
library(car)
library(forcats)
library(RColorBrewer)
library(randomForest)
library(gbm)
library(Cubist)
library(kableExtra)
library(e1071)
#load data
train <- read.csv("TrainingData.csv")
#review
glimpse(train)
library(ggplot2)
library(readr)
library(DataExplorer)
library(summarytools)
library(Amelia)
library(VIM)
library(dplyr)
library(forecast)
library(tidyr)
library(mice)
library(corrplot)
library(MASS)
library(earth)
library(RANN)
library(caret)
library(car)
library(forcats)
library(RColorBrewer)
library(randomForest)
library(gbm)
library(Cubist)
library(kableExtra)
library(e1071)
#load data
train <- read.csv("TrainingData.csv")
#review
glimpse(train)
#NA counts by column
#sapply(train, function(x) sum(is.na(x)))
VIM::aggr(train, numbers=T, sortVars=T, bars = FALSE, border= 'white',
cex.axis = .6,
ylab=c("Proportion of NAs", "Combinations"))
print(dfSummary(train), file = '~/train_summary.html')
summary(train)
ggplot(train, aes(x=reorder(Brand.Code, Brand.Code, function(x)-length(x)))) +
geom_bar() +  labs(x='Brand.Code')+
labs(title= 'Brand.Code Distribution')+
theme_minimal()
DataExplorer::plot_histogram(train, nrow = 3L, ncol = 4L)
numeric_values <- train
numeric_values<- numeric_values %>%
select_if(is.numeric) %>%
na.omit()
train_cor <- cor(numeric_values)
#train_cor_filtered <- cor(train_cor[,-findCorrelation(train_cor,cutoff = 0.9,exact = TRUE)])
corrplot(train_cor, tl.col = 'black', col=brewer.pal(n=10, name="RdYlBu"))
ph_corr <- as.data.frame(cor(numeric_values[-1], numeric_values$PH))
ph_corr <- cbind("Predictor" = rownames(ph_corr), ph_corr)
rownames(ph_corr) <- 1:nrow(ph_corr)
ph_corr <- ph_corr[-24,]
ggplot(ph_corr, aes(x=fct_reorder(factor(Predictor),V1), y = (V1))) +
geom_col(position="dodge", fill="steelblue") +
coord_flip()+
labs(title="Correlations to pH",
x="Predictors",
y="Correlation Coefficient")+
geom_text(aes(label = round(V1,2)), colour = "black", size = 3,
position = position_stack(vjust = 0.6))+
theme_minimal()
nzv<- nearZeroVar(train, saveMetrics= TRUE)
filter(nzv, nzv=="TRUE")
#add name to empty string
train$Brand.Code[train$Brand.Code == ""] <- "Unknown"
#convert variable type to factor
train <- train %>%
dplyr::mutate(Brand.Code = factor(Brand.Code,
levels = c('A','B','C','D', 'Unknown'),
ordered = FALSE))
set.seed(624)
#remove pH from the train data set in order to only transform the predictors
train_features <- train %>%
dplyr::select(-c(PH))
#remove nzv, correlated values, center and scale, apply BoxCox for normalization
preProc <- preProcess(train_features, method=c("knnImpute","nzv","corr",
"center", "scale", "BoxCox"))
#get the transformed features
preProc_train <- predict(preProc, train_features)
#add the PH response variable to the preProcessed train features
preProc_train$PH <- train$PH
#there are 4 NAs in the PH response variable, those need to be removed
preProc_train <- na.omit(preProc_train)
#partition data for evaluation
training_set <- createDataPartition(preProc_train$PH, p=0.8, list=FALSE)
train_data <- preProc_train[training_set,]
eval_data <- preProc_train[-training_set,]
#Remove PH from sets to feed models
set.seed(222)
y_train <- subset(train_data, select = -c(PH))
y_test <- subset(eval_data, select = -c(PH))
#generate model
linear_model <- train(x= y_train, y= train_data$PH,
method='lm',
trControl=trainControl(method = "cv", number = 10))
#evaluate model
lmPred <- predict(linear_model, newdata = y_test)
lmResample <- postResample(pred=lmPred, obs = eval_data$PH)
set.seed(222)
#generate model
pls_model <- train(y_train, train_data$PH,
method='pls',
metric='Rsquared',
tuneLength=10,
trControl=trainControl(method = "cv",  number = 10))
#evaluate model metrics
plsPred <-predict(pls_model, newdata=y_test)
plsReSample <- postResample(pred=plsPred, obs = eval_data$PH)
set.seed(222)
#generate model
initial <- lm(PH ~ . , data = train_data)
AIC_model <- stepAIC(initial, direction = "both",
trace = 0)
#evaluate model metrics
AIC_Pred <-predict(AIC_model, newdata=y_test)
aicResample <- postResample(pred=AIC_Pred, obs=eval_data$PH)
display <- rbind("Linear Regression" = lmResample,
"Stepwise AIC" = aicResample,
"Partial Least Squares" = plsReSample)
display %>% kable() %>% kable_paper()
set.seed(624)
knnModel <- train(PH~., data = train_data,
method = "knn",
preProc = c("center", "scale"),
tuneLength = 10)
#knnModel
knnPred <- predict(knnModel, newdata = eval_data)
knn_metrics <- postResample(pred = knnPred, obs = eval_data$PH)
#knn_metrics
set.seed(624)
tc <- trainControl(method = "cv",
number = 5,
classProbs = T)
svmModel <- train(PH~., data = train_data,
method = "svmRadial",
preProcess = c("BoxCox","center", "scale"),
trControl = tc,
tuneLength = 9)
#svmModel
svmPred <- predict(svmModel, newdata = eval_data)
svm_metrics <- postResample(pred = svmPred, obs = eval_data$PH)
#svm_metrics
set.seed(624)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
mars <- train(PH~., data = train_data,
method = "earth",
tuneGrid = marsGrid,
trControl = trainControl(method = "cv"))
#mars
marsPred <- predict(mars, newdata = eval_data)
mars_metrics <- postResample(pred = marsPred, obs = eval_data$PH)
#mars_metrics
knnModel_pred <- knnModel %>% predict(eval_data)
# Model performance metrics
knn_Accuracy <- data.frame(
Model = "k-Nearest Neighbors",
RMSE = caret::RMSE(knnModel_pred,eval_data$PH),
Rsquare = caret::R2(knnModel_pred,eval_data$PH))
pred_svm <- svmModel %>% predict(eval_data)
# Model SVM performance metrics
SMV_Acc <- data.frame(
Model = "Support Vector Machine",
RMSE = caret::RMSE(pred_svm, eval_data$PH),
Rsquare = caret::R2(pred_svm, eval_data$PH)
)
#summary(marsTuned)
# Make MARS predictions
pred_mars <- mars %>% predict(eval_data)
# Model MARS performance metrics
MARS_Acc <- data.frame(
Model = "MARS Tuned",
RMSE = caret::RMSE(pred_mars, eval_data$PH),
Rsquare = caret::R2(pred_mars, eval_data$PH)
)
names(MARS_Acc)[names(MARS_Acc) == 'y'] <- "Rsquare"
#rbind(knn_Accuracy,SMV_Acc,MARS_Acc)
### code for the plot
par(mar = c(4, 4, 4, 4))
par(mfrow=c(2,2))
plot(knnModel_pred, eval_data$PH, ylab="Observed", col = "red")
abline(0, 1, lwd=2)
plot(pred_svm, eval_data$PH, ylab="Observed", col = "dark green")
abline(0, 1, lwd=2)
plot(pred_mars, eval_data$PH, ylab="Observed", col = "blue")
abline(0, 1, lwd=2)
mtext("Observed Vs. Predicted  - Non - Linear Models with Reduced Predictor Set", side = 3, line = -2, outer = TRUE)
set.seed(624)
NNModel <- avNNet(PH~., data = train_data,
size = 5,
decay = 0.01,
linout = TRUE,
trace = FALSE,
maxit = 500)
#NNModel
pred_NNModel <- NNModel %>% predict(eval_data)
nn_metrics <- postResample(pred = pred_NNModel, obs = eval_data$PH)
#nn_metrics
rbind( "KNN" = knn_metrics,
"SVM" = svm_metrics,
"MARS" = mars_metrics,
"Neural Network" = nn_metrics) %>%
kable() %>% kable_paper()
varImp(NNModel) %>% head(10)
ggplot(train_data, aes(Oxygen.Filler, PH)) +
geom_point()
ggplot(train_data, aes(Mnf.Flow	, PH)) +
geom_point()
ggplot(train_data, aes(Bowl.Setpoint	, PH)) +
geom_point()
set.seed(624)
#remove pH from the train data set in order to only transform the predictors
train_features <- train %>%
dplyr::select(-c(PH))
#remove nzv, correlated values, impute NAs
preProc <- preProcess(train_features, method=c("knnImpute","nzv","corr"))
#get the transformed features
preProc_train <- predict(preProc, train_features)
#add the PH response variable to the preProcessed train features
preProc_train$PH <- train$PH
#there are 4 NAs in the PH response variable, those need to be removed
preProc_train <- na.omit(preProc_train)
#partition data for evaluation
training_set <- createDataPartition(preProc_train$PH, p=0.8, list=FALSE)
train_data_rf <- preProc_train[training_set,]
eval_data_rf <- preProc_train[-training_set,]
train_rf_predictors <- train_data_rf[-c(26)]
eval_rf_predictors <- eval_data_rf[-c(26)]
set.seed(624)
#fit the model
rf_model <- randomForest(train_rf_predictors, train_data_rf$PH,
importance = TRUE, ntrees = 500)
#metrics
rfPred <- predict(rf_model, newdata = eval_rf_predictors)
rf_metrics <- postResample(pred = rfPred, obs = eval_data_rf$PH)
#rf_metrics
set.seed(624)
#fit model
gbm_model <- train(train_rf_predictors, train_data_rf$PH,
method = "gbm",
verbose = FALSE)
#metrics
gbmPred <- predict(gbm_model, newdata = eval_rf_predictors)
gbm_metrics <- postResample(pred = gbmPred, obs = eval_data_rf$PH)
#gbm_metrics
set.seed(6)
#fit model
cube_model <- train(train_rf_predictors, train_data_rf$PH,
method = "cubist",
verbose = FALSE)
#metrics
cubePred <- predict(cube_model, newdata =  eval_rf_predictors)
cube_metrics <- postResample(pred = cubePred, obs = eval_data_rf$PH)
#cube_metrics
rbind("Random Forest" = rf_metrics,
"Boosted Trees" = gbm_metrics,
"Cubist" = cube_metrics) %>%
kable() %>% kable_paper()
all_metrics <- as.data.frame(rbind("Linear Regression" = lmResample,
"Neural Network" = nn_metrics,
"Cubist" = cube_metrics))
all_metrics[order(all_metrics$RMSE),] %>%
kable() %>% kable_paper()
plot(varImp(cube_model), top=10)
#load test data
test <- read.csv("TestData.csv")
#review
glimpse(test)
#subset features from response
test_features <- test %>%
dplyr::select(-c(PH))
print(dfSummary(test), file = '~/test_summary.html')
summary(test)
ggplot(test, aes(x=reorder(Brand.Code, Brand.Code, function(x)-length(x)))) +
geom_bar() +  labs(x='Brand.Code')+
labs(title= 'Brand.Code Distribution')+
theme_minimal()
#NAs in Test data set
#NA counts by column
#sapply(test, function(x) sum(is.na(x)))
VIM::aggr(test_features, numbers=T, sortVars=T, bars = FALSE, border= 'white',
cex.axis = .6,
ylab=c("Proportion of NAs", "Combinations"))
#prep data- change Brand.Code to factor
#add name to empty string
test_features$Brand.Code[test_features$Brand.Code == ""] <- "Unknown"
#convert variable type to factor
test_features <- test_features %>%
dplyr::mutate(Brand.Code = factor(Brand.Code,
levels = c('A','B','C','D', 'Unknown'),
ordered = FALSE))
set.seed(624)
#impute NAs
preProc_test <- preProcess(test_features, method=c("knnImpute"))
#get the transformed features
test_features <- predict(preProc_test, test_features)
#identify the variables that need to be removed
setdiff(colnames(test_features),colnames(train_rf_predictors))
#remove the variables identified above
test_features <- test_features %>%
dplyr::select(-c("Hyd.Pressure1","Hyd.Pressure3","Filler.Level",
"Filler.Speed","Density","Balling","Balling.Lvl" ))
predict <- round(predict(cube_model, newdata=test_features),2)
test_features$PH <- predict
write_excel_csv(predict, "Predictions.csv")
predict <- as.data.frame(round(predict(cube_model, newdata=test_features),2))
write_excel_csv(predict, "Predictions.csv")
pH <- as.data.frame(predict)
predict <- round(predict(cube_model, newdata=test_features),2)
pH <- as.data.frame(predict)
#test_features$PH <- predict
write_excel_csv(pH, "Predictions.csv")
predict <- round(predict(cube_model, newdata=test_features),2)
#pH <- as.data.frame(predict)
test_features$PH <- predict
write_excel_csv(test_features, "Predictions.csv")
pH$pH <- as.data.frame(predict)
write_excel_csv(pH, "Predictions.csv")
pH <- as.data.frame(predict)
View(pH)
rename(pH, pH = predict)
write_excel_csv(pH, "Predictions.csv")
predict <- round(predict(cube_model, newdata=test_features),2)
pH <- as.data.frame(predict)
rename(pH, pH = predict)
#test_features$PH <- predict
write_excel_csv(pH, "Predictions.csv")
predict <- round(predict(cube_model, newdata=test_features),2)
pH <- as.data.frame(predict)
pH<- rename(pH, pH = predict)
#test_features$PH <- predict
write_excel_csv(pH, "Predictions.csv")
library(tidyverse)
read.csv("C:\Users\gabri\Downloads\archive\Fitabase Data 4.12.16-5.12.16\dailyActivity_merged")
read.csv("C:/Users/gabri/Downloads/archive/Fitabase Data 4.12.16-5.12.16/dailyActivity_merged")
read.csv(C:/Users/gabri/Downloads/archive/Fitabase Data 4.12.16-5.12.16/dailyActivity_merged)
install.packages(c("BiocManager", "bookdown", "broom", "bslib", "callr", "car", "caret", "classInt", "cli", "corrr", "cvms", "datawizard", "DBI", "dbplyr", "DescTools", "dials", "DT", "dtplyr", "e1071", "effects", "ellipse", "embed", "estimability", "evaluate", "faraway", "farver", "FNN", "fontawesome", "forcats", "forecast", "furrr", "future", "gbm", "generics", "ggforce", "gld", "globals", "googlesheets4", "gtools", "hardhat", "haven", "Hmisc", "hms", "htmlTable", "htmltools", "httr", "igraph", "infer", "insight", "inspectdf", "ipred", "janeaustenr", "keras", "kernlab", "knitr", "latticeExtra", "lme4", "lmom", "lobstr", "MASS", "mnormt", "modeldata", "modelr", "multcomp", "nloptr", "openssl", "packrat", "palmerpenguins", "parallelly", "parameters", "parsnip", "partykit", "patchwork", "pillar", "pkgload", "plotmo", "pls", "processx", "progressr", "proxy", "ps", "psych", "qap", "quantreg", "randomForest", "ranger", "Rcpp", "RcppArmadillo", "RCurl", "reactable", "readxl", "rearrr", "recipes", "reprex", "reticulate", "rgdal", "rlang", "rmarkdown", "rpart.plot", "rsample", "rsconnect", "rstudioapi", "rvest", "s2", "sandwich", "sass", "scales", "seriation", "sf", "shiny", "sp", "stringi", "stringr", "strucchange", "summarytools", "tensorflow", "tibble", "tidycensus", "tidymodels", "tidytext", "tidyverse", "tigris", "timeDate", "timetk", "tinytex", "tsibble", "TSP", "tune", "tweenr", "urca", "uwot", "vcd", "VIM", "viridisLite", "workflows", "workflowsets", "xfun", "yardstick"))
getwd()
setwd("~/CUNY SPS/DATA_608/DATA608")
setwd("~/CUNY SPS/DATA_608/DATA608")
getwd()
